{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe9e084",
   "metadata": {},
   "source": [
    "# PyTorch Neural Network Classification\n",
    "\n",
    "This notebook closely follows the material available at learnpytorch.io [[1]](https://www.learnpytorch.io/) with occasional refactoring and extension for consistency of style and to make connections with other parts of the package. It is also more extensive on examples and does less revisit of lower level concepts once discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb367e",
   "metadata": {},
   "source": [
    "### Classification Problems\n",
    "\n",
    "A classification problem refers to the identification of which class an observation belongs to amongst a set of classes. We distinguish two major cases.\n",
    "\n",
    "* Binary. There are only two classes to choose from.\n",
    "* Multi-class. There are more than two classes.\n",
    "\n",
    "Note that while binary classification could be a subcase of multi-class classification if the definition is relaxed, however, considering the binary case individually allows for the development of specialized methodology that is often more powerful.\n",
    "\n",
    "There is also a variant named multi-label classification where multiple nonexclusive labels can be assigned to the target observation. Observe that classification here is not an entirely correct term as classes are mutually exclusive by definition in most mathematical terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b495029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import make_circles\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ce18c",
   "metadata": {},
   "source": [
    "### Sample problem\n",
    "\n",
    "We will rely on `sklearn`'s `make_circles` function to generate toy dataset of concentric circles in two dimension. We will control the number of points generated in total for the two circles using a prefixed sample size of 1000 and add a standard normal variation of 0.03. The random state is also fixed for reproducibility of the problem. The output is the coordinates and a classification using 0's and 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b1416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "points, classification = make_circles(\n",
    "    sample_size,\n",
    "    noise=0.03,\n",
    "    random_state=42,\n",
    ")\n",
    "print(points[:10])\n",
    "print()\n",
    "print(classification[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a87b7",
   "metadata": {},
   "source": [
    "We can readily visualize the circles using a scatter plot in a creative way. The first dimension of the points in `x` will serve as the horizontal input, similarly, the vertical line will be marked with the second dimension. The colormap will be `RdYlBu` which basically colors based on values of the unit interval, assigning red to 0, blue to 1, and *yellowish* to mid range numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90839dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(\n",
    "    x=points[:, 0], \n",
    "    y=points[:, 1], \n",
    "    c=classification,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae8081",
   "metadata": {},
   "source": [
    "Recall that in a generic machine learning workflow, we first take historical data then turn it into tensors. Our data at the moment is represented by `numpy` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(points))\n",
    "print(type(classification))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.from_numpy(points).type(torch.float)\n",
    "classification = torch.from_numpy(classification).type(torch.float)\n",
    "print(points[:10])\n",
    "print()\n",
    "print(classification[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f9e80",
   "metadata": {},
   "source": [
    "The next step would be to split the data into training and testing sets. The `sklearn` package offers a randomized split by means of the `train_test_split` utility. This needs to be used carefully, see [[2]](https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50), but in our case we ignore the finer details. We pick a standard Pareto split of 80 and 20 percents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb72833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_input, test_input, train_output, test_output = train_test_split(\n",
    "    points,\n",
    "    classification,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c5416",
   "metadata": {},
   "source": [
    "We continue with building the model itself. In order to do so, a short detour is needed into the finer architecture of it. We will make use of a hyperparameter, the number of hidden neurons. Instead of going through a single layer of points to classification, our model will consist of two linear layers, the first which is capable of taking in 2 features, standing for the x and y coordinates, and then outputs 5 features, these will serve as the hidden neurons. The second linear layer will build on top of these 5 features as input and outputs a single feature, the classification itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleModelLinearLayers(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5)\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1)\n",
    "    \n",
    "    def forward(self, points):\n",
    "        return self.layer_2(self.layer_1(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018cebb",
   "metadata": {},
   "source": [
    "Let us make optional use of Cuda if available and create an instance of the model, then make an untrained prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CircleModelLinearLayers().to(device)\n",
    "with torch.inference_mode():\n",
    "    prediction = model(train_input.to(device)).squeeze()\n",
    "print(prediction[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5d6852",
   "metadata": {},
   "source": [
    "Notice that the outputs are not in the form of a classification. These are raw outputs from the two-layer sequential linear model and referred to as logits, see [[3]](https://en.wikipedia.org/wiki/Logit) for more details. It is essentially the quantile function of the logistic distribution or in other formulation it is the inverse of the cummulative distribution function of the logistic distribution. The cummulative distribution function has its values interpreted as probabilities, consequently, applying the inverse of the logit function would transform the raw output into values that are interpreted as probabilities. The inverse of the logit function is the expit function and can be readily used by referencing the `sigmoid` function in `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = torch.sigmoid(prediction)\n",
    "print(probabilities[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073324de",
   "metadata": {},
   "source": [
    "To obtain a particular classification output, we still need to map the probabilities to 0's and 1's. As a rule of thumb, we can simply go with 0 for probabilities less than 0.5 and 1 for anything at least 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = torch.round(probabilities)\n",
    "print(classification[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44ffd6",
   "metadata": {},
   "source": [
    "The training will require a loss function and an optimizer. Optimizers are mostly problem space agnostic, however, the loss function needs to be adjusted to the scenario. In the case of a linear regression the loss was defined as the mean absolute deviation. This would be not appropriate as we would basically value perfect matches only. For binary classification problems, the binary cross entropy is one of the standard choices as the loss function, see [[4]](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcda878",
   "metadata": {},
   "source": [
    "We are left to build the training and testing loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_input, test_input = train_input.to(device), test_input.to(device)\n",
    "train_output, test_output = train_output.to(device), test_output.to(device)\n",
    "epoch_count = 100\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "    model.train()\n",
    "    train_logits = model(train_input).squeeze()\n",
    "    train_classification = torch.round(torch.sigmoid(train_logits))\n",
    "    \n",
    "    train_loss = loss_function(train_logits, train_output)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(test_input).squeeze()\n",
    "        test_classification = torch.round(torch.sigmoid(test_logits))\n",
    "        test_loss = loss_function(test_logits, test_output)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4845a82",
   "metadata": {},
   "source": [
    "Observe that the loss is not decreasing visibily as the training process progresses. We have multiple options to improve the model by tweaking the hyperparameters.\n",
    "\n",
    "* More layers\n",
    "* More hidden units\n",
    "* More epochs\n",
    "* Activation functions to introduce non-linearity\n",
    "* Adjust learning rate\n",
    "* Change loss function\n",
    "* Use transfer learning.\n",
    "\n",
    "Here, the most probable issue is that our modeling is working with lines, consequently, it can \"cut\" data along lines. However, the data is near-perfectly balanced in terms of the classification (circular) and this is not going to lead anywhere. We need to introduce non-linearity. This can be done through a rectifier, see [[5]](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)). There is an out of the box solution by means of `torch`'s `ReLU` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircleModelNonLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5)\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, points):\n",
    "        return self.layer_2(self.relu(self.layer_1(points)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d93fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CircleModelNonLinear().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ea254",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "train_input, test_input = train_input.to(device), test_input.to(device)\n",
    "train_output, test_output = train_output.to(device), test_output.to(device)\n",
    "epoch_count = 100\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "    model.train()\n",
    "    train_logits = model(train_input).squeeze()\n",
    "    train_classification = torch.round(torch.sigmoid(train_logits))\n",
    "    \n",
    "    train_loss = loss_function(train_logits, train_output)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(test_input).squeeze()\n",
    "        test_classification = torch.round(torch.sigmoid(test_logits))\n",
    "        test_loss = loss_function(test_logits, test_output)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34737b",
   "metadata": {},
   "source": [
    "Slightly better, but not much. We may increase the epoch count as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7355f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CircleModelNonLinear().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_input, test_input = train_input.to(device), test_input.to(device)\n",
    "train_output, test_output = train_output.to(device), test_output.to(device)\n",
    "epoch_count = 10000\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "    model.train()\n",
    "    train_logits = model(train_input).squeeze()\n",
    "    train_classification = torch.round(torch.sigmoid(train_logits))\n",
    "    \n",
    "    train_loss = loss_function(train_logits, train_output)\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(test_input).squeeze()\n",
    "        test_classification = torch.round(torch.sigmoid(test_logits))\n",
    "        test_loss = loss_function(test_logits, test_output)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c1e4b",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Learn PyTorch for Deep Learning: Zero to Mastery book, accessed online on 2023.04.25 at https://www.learnpytorch.io/\n",
    "\n",
    "[2] Mayukh Bhattacharyya, 3 Things You Need To Know Before You Train-Test Split, Towards Data Science article, accessed online on 2023.04.25 at https://towardsdatascience.com/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50\n",
    "\n",
    "[3] Logit, Wikipedia article, accessed online on 2023.04.25 at https://en.wikipedia.org/wiki/Logit\n",
    "\n",
    "[4] Daniel Godoy, Understanding binary cross-entropy/log loss: a visual explanation, Towards Data Science article, accessed online on 2023.04.25 at https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "[5] Rectifier, Wikipedia article, accessed online on 2023.04.25 at https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
