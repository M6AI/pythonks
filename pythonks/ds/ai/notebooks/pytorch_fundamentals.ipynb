{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842b0185",
   "metadata": {},
   "source": [
    "# PyTorch Fundamentals\n",
    "\n",
    "This notebook closely follows the material available at learnpytorch.io [[1]](https://www.learnpytorch.io/) with occassional refactoring and extension for consistency of style and to make connections with other parts of the package. It is also more extensive on examples and does less revisit of lower level concepts once discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9341d017",
   "metadata": {},
   "source": [
    "### About PyTorch\n",
    "\n",
    "PyTorch is an open source machine and/or, depending on accepted classification, deep learning framework with a highly optimized tensor workflow using both CPU and GPU for computation. It is widely used both in industry and academia, and, as of 2022, it is the most used deep learning framework on Papers with Code [[2]](https://paperswithcode.com/trends). Its design philosophy is based around the following principles.\n",
    "- Usability over performance\n",
    "- Simple over easy\n",
    "- Python first with best in class language interoperability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e560231",
   "metadata": {},
   "source": [
    "### Install PyTorch and confirm version\n",
    "\n",
    "The recommended way to install PyTorch in a virtual environment and CUDA 11.7 support is the following.\n",
    "\n",
    "`pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117`\n",
    "\n",
    "The version of `torch` references both the PyTorch version and the linked CUDA installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45617f",
   "metadata": {},
   "source": [
    "In order to check if there is an Nvidia GPU, we can run the `nvidia_smi` command on the command line. From within the notebook we can achieve the same by prefixing it with a bang `!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71720d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e58d8b",
   "metadata": {},
   "source": [
    "**Note.** If the CUDA version listed differs from the one that shipped with the PyTorch binaries, then it may not necessarily be a problem. For instance, 11.7 version coming with PyTorch will be able to work with 12.1 on the GPU side. *This was tested with an NVIDIA GeForce GTX 1050 Ti 4GB GPU and driver version of 531.41.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746624f4",
   "metadata": {},
   "source": [
    "Cuda availability can be checked explicitly leaving it open to make the usage of the GPU conditional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0beeb5",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are higher-order generalizations of vector and matrix concepts and as such are implemented using the abstraction of higher-dimensional arrays. An object $T$ is called an $n$-order or $n$-dimensional tensor if there are vector spaces $V_i$ of dimensions $d_i$ for $i = 0, 1, \\dots, n - 1$ such that\n",
    "\n",
    "$$\n",
    "T = V_0 \\times V_1 \\times \\dots \\times V_{n - 1}\n",
    "$$\n",
    "\n",
    "In PyTorch, the vector spaces have finitely many elements by nature (finiteness of representation) and can correspond to one of the supported data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_types = {\n",
    "    \"32-bit floating point\": (torch.float32, torch.float),\n",
    "    \"64-bit floating point\": (torch.float64, torch.double),\n",
    "    \"64-bit complex\": (torch.complex64, torch.cfloat),\n",
    "    \"128-bit complex\": (torch.complex128, torch.cdouble),\n",
    "    \"16-bit floating point (1 sign, 5 exponents, 10 significand )\": (torch.float16, torch.half),\n",
    "    \"16-bit Brain floating point (1 sign, 8 exponent, 7 significand)\": (torch.bfloat16,),\n",
    "    \"8-bit integer (unsigned)\": (torch.uint8,),\n",
    "    \"8-bit integer (signed)\": (torch.int8,),\n",
    "    \"16-bit integer (signed)\": (torch.int16, torch.short),\n",
    "    \"32-bit integer (signed)\": (torch.int32, torch.int),\n",
    "    \"64-bit integer (signed)\": (torch.int64, torch.long),\n",
    "    \"Boolean\": torch.bool,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99cb1eb",
   "metadata": {},
   "source": [
    "Perhaps not surprisingly, standard nested `list` objects that respect the homogeneous shape requirements, can be used to construct the first `Tensor` examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5df745",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = torch.tensor([1, 0])\n",
    "matrix = torch.tensor(\n",
    "    [\n",
    "        [0., 1.],\n",
    "        [1., 0.],\n",
    "    ]\n",
    ")\n",
    "tensor = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [True, False],\n",
    "            [False, True],\n",
    "        ],\n",
    "        [\n",
    "            [True, True],\n",
    "            [False, False],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "for t in [vector, matrix, tensor]:\n",
    "    print(f\"{t}\\n\\n\\ttensor of type {t.dtype}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f217993",
   "metadata": {},
   "source": [
    "**Note.** Above examples also show the default `dtype` chosen for Python types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3fb1d",
   "metadata": {},
   "source": [
    "PyTorch supports not-a-number values in the form of `float(\"nan\")` and also positive and negative infinite values `float(\"Inf\")` and `float(\"-Inf\")`, respectively.\n",
    "It worth mentioning that the Boolean type in PyTorch is not nulleable, NaN values are considered `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([float(item) for item in [\"nan\", \"Inf\", \"-Inf\"]], dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2baedf",
   "metadata": {},
   "source": [
    "In mathematics, the most straighforward explanation of a tensor would be a multilinear mapping. However, in PyTorch, tensors are meant to be understood more as real world inputs mapped onto supported data types, at least initially. For example, a 2 by 2 pixels image can be  considered as a tensor of order 3 over integer-type domains of dimensions 2, 2, and 3, where the first and second orders with dimension 2 each stand for the height and the width, respectively, and the third order of dimension 3 reflects the color channel (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d23243",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [0.1, 0.2, 0.3],  # RGB of pixel (0, 0)\n",
    "            [0.2, 0.2, 0.3],  # RGB of pixel (0, 1)\n",
    "        ],\n",
    "        [\n",
    "            [0.4, 0.5, 0.5],  # RGB of pixel (1, 0)\n",
    "            [0.5, 0.6, 0.6],  # RGB of pixel (1, 1)\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d1dde",
   "metadata": {},
   "source": [
    "### Deterministic tensor constructors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4848f",
   "metadata": {},
   "source": [
    "Let us begin with the convention that scalars are thought of as tensors of dimension 0 rather than dimension 1 over any of the allowed domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22293205",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = torch.tensor(0)\n",
    "print(f\"{scalar} is an object {type(scalar)} of dimension {scalar.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654b9f8",
   "metadata": {},
   "source": [
    "We have already seen the conversion of Python types into tensors by means of `torch.tensor`. This is fairly inefficient due to the performance of Python objects and also rarely needed as under normal circumstances inputs are flowing in through I/O streams. We may additionally need to create tensors of special forms to supplement the flow of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8707723",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((2, 2))\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones((3, 2, 1))\n",
    "print(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "arith_range = torch.arange(30).reshape(2, 3, 5)\n",
    "print(arith_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eed38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "equispaced = torch.linspace(0, 9, 10).reshape(2, 5)\n",
    "print(equispaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaead2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "equiscaled = torch.logspace(0, 1, 10).reshape(2, 5)\n",
    "print(equiscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ed28b",
   "metadata": {},
   "source": [
    "### Tensor algebra\n",
    "\n",
    "Tensors generalize matrices not only in terms of representation, but also in terms of operations defined for them. PyTorch tensors support pointwise addition, subtraction, multiplication, and division. Also, as expected from vectors spaces, they support scalar multiplication, matrix multiplication along axes of identical dimensions, and also implement the transpose operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros((2, 3))\n",
    "ones = torch.ones((3, 2))\n",
    "transponent = ones.t()\n",
    "print(f\"Transponent of\\n{ones}\\n  is:\\n{transponent}\")\n",
    "print()\n",
    "print(f\"Sum of\\n{zeros}\\n  and\\n{transponent}\\n  is:\\n{zeros + transponent}\")\n",
    "print()\n",
    "print(f\"Difference of\\n{zeros}\\n  and\\n{transponent}\\n  is:\\n{zeros - transponent}\")\n",
    "print()\n",
    "print(f\"Pointwise product of\\n{zeros}\\n  and\\n{transponent}\\n  is:\\n{zeros * transponent}\")\n",
    "print()\n",
    "print(f\"Pointwise quotient of\\n{zeros}\\n  and\\n{transponent}\\n  is:\\n{zeros / transponent}\")\n",
    "print()\n",
    "print(f\"Double of\\n{ones}\\n  is:\\n{2 * ones}\")\n",
    "print()\n",
    "print(f\"Matrix product of\\n{zeros}\\n  and\\n{ones}\\n  is:\\n{torch.matmul(zeros, ones)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f0a1a",
   "metadata": {},
   "source": [
    "Basic concepts of determinant and matrix inversion are also lifted to their higher dimensional analogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d681703",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.linspace(0, 1, 16).reshape(2, 2, 2, 2)\n",
    "print(f\"(Hyper)determinant of\\n{tensor}\\n  is:\\n{tensor.det()}\")\n",
    "print()\n",
    "print(f\"Inverse of\\n{tensor}\\n  is:\\n{tensor.inverse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e080c",
   "metadata": {},
   "source": [
    "**Note.** A legitim question at this point is that *\"under what circumstances the determinant and inverse can be thought of as useful measures of input data?\"*. In fact, these become more relevant once the tensor is thought of as a multilinear mapping or in an overly simplified manner as a state transition expression. The determinant helps understanding enlargening/shrinking effects while the inverse can be considered a reversal of transition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9826f",
   "metadata": {},
   "source": [
    "Comparison is understood pontwise between tensors of identical shape. It is also possible to compare against scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Positions where\\n{zeros}\\n  is less than\\n{transponent}\\n  are:\\n{zeros < transponent}\")\n",
    "print()\n",
    "print(f\"Positions where\\n{zeros}\\n  is greater than\\n{transponent}\\n  are:\\n{zeros > transponent}\")\n",
    "print()\n",
    "print(f\"Positions where\\n{zeros}\\n  is equal to\\n{transponent}\\n  are:\\n{zeros == transponent}\")\n",
    "print()\n",
    "print(f\"Positions where\\n{zeros}\\n  is not equal to\\n{transponent}\\n  are:\\n{zeros != transponent}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1089db",
   "metadata": {},
   "source": [
    "Boolean operations apply pointwise too; the boolean value of tensors themselves are ambiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.arange(9).reshape(3, 3)\n",
    "print(f\"Positions where\\n{tensor}\\n  is less than 3 or larger than 6 are\\n{(tensor < 3) | (tensor > 6)}\")\n",
    "print()\n",
    "print(f\"Positions where\\n{tensor}\\n  is less than 6 and larger than 3 are\\n{(tensor > 3) & (tensor < 6)}\")\n",
    "print()\n",
    "print(f\"Positions where\\n{tensor}\\n  is less than or equal to 3 or larger than or equal to 6\\n{~((tensor > 3) & (tensor < 6))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b187c2",
   "metadata": {},
   "source": [
    "### Stochastic tensor constructors\n",
    "\n",
    "Often times, we need to sample values of a tensor from some probability distribution. PyTorch offers stochastic constructors for many common distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_tensor = torch.rand(2, 3, 4)\n",
    "print(uniform_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unconventional constructor using an pointwise free p parameter\n",
    "# It does have to do with Markov-chains and evolutionary processes\n",
    "bernoulli_tensor = torch.bernoulli(\n",
    "    0.5 * torch.ones((10, 10))  # Fix p to be 0.5 across all the sampling\n",
    ")\n",
    "print(bernoulli_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_tensor = torch.normal(0, 1, (5, 5))\n",
    "print(gaussian_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same comment applies as for Bernoulli sampling\n",
    "poisson_tensor = torch.poisson(\n",
    "    torch.ones((5, 5))  # Fix lambda to be 1 across all the sampling\n",
    ")\n",
    "print(poisson_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848b540",
   "metadata": {},
   "source": [
    "In case sampling scenarios are to be repeated, the random seed can be fixed to ensure identical outputs. Note that the seed moves away during each sampling according to the generator process and needs to be reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533fe433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed():\n",
    "    torch.manual_seed(seed=1)\n",
    "\n",
    "reset_seed()\n",
    "print(torch.rand(3, 3))\n",
    "reset_seed()\n",
    "print(torch.rand(3, 3))\n",
    "reset_seed()\n",
    "print(torch.rand(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc592c22",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Tensors implement the standard accessors by position: explicit indexing in one or multiple dimensions and slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.arange(24).reshape(2, 3, 4)\n",
    "print(f\"Tensor example used is:\\n{tensor}\\nWe are going to think along x, y, and z axes respectively.\")\n",
    "print()\n",
    "print(f\"Accessing first element along axis x:\\n{tensor[0]}\")\n",
    "print()\n",
    "print(f\"Accessing first element along axis y:\\n{tensor[:, 0]}\")\n",
    "print()\n",
    "print(f\"Accessing first element along axis z:\\n{tensor[:, :, 0]}\")\n",
    "print()\n",
    "print(f\"Accessing first element along axis x and y:\\n{tensor[0, 0]}\")\n",
    "print()\n",
    "print(f\"Accessing first element along axis x and z:\\n{tensor[0, :, 0]}\")\n",
    "print()\n",
    "print(f\"Accessing first element along axis y and z:\\n{tensor[:, 0, 0]}\")\n",
    "print()\n",
    "print(f\"Accessing first element along axis x, second and third along y:\\n{tensor[0, 1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1312d8",
   "metadata": {},
   "source": [
    "In addition, tensors support boolean masking like NumPy arrays and Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.normal(0, 1, (3, 3, 3))\n",
    "print(f\"Tensor example used is:\\n{tensor}\")\n",
    "print()\n",
    "print(f\"Elements less than 0 are:\\n{tensor[tensor < 0]}\")\n",
    "print()\n",
    "print(f\"Elements between -1 and 1:\\n{tensor[(-1 < tensor) & (tensor < 1)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5411f",
   "metadata": {},
   "source": [
    "### Aggregation\n",
    "\n",
    "Since tensors are used for representation of input data, in many scenarios, we are interested in aggregate measures, more precisely statistics, along various axes. Most common descriptives are implemented as methods of `Tensor` objects. Note that some aggregates return a corresponding type that is more of a view with a tensor of indices supplementing it (for instance, minimum, maximum, median, etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.normal(0, 1, (5, 5, 5))\n",
    "\n",
    "print(f\"Tensor example used is:\\n{sample}\")\n",
    "print()\n",
    "print(f\"Mean across all three axes is:\\n{sample.mean()}\")\n",
    "print()\n",
    "print(f\"Means across x axis are:\\n{sample.mean(0)}\")\n",
    "print()\n",
    "print(f\"Maximums across y axis are:\\n{sample.max((1))}\")\n",
    "print()\n",
    "print(f\"Minimums across z axis are:\\n{sample.min(2)}\")\n",
    "print()\n",
    "print(f\"Standard deviations across x and y axes are:\\n{sample.std((0, 1))}\")\n",
    "print()\n",
    "print(f\"Medians across x and z axes are:\\n{sample.median((0))[0].median(1)}\")\n",
    "print()\n",
    "print(f\"Cummulative sums across y and z axes are:\\n{sample.cumsum(1)[0].cumsum(1)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80ffb9",
   "metadata": {},
   "source": [
    "### CPU vs. GPU\n",
    "\n",
    "When a GPU is available, then ideally, we should be able to leverage it for computation. This requires the tensor to be stored on the GPU device rather than the CPU which is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91011884",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_on_cpu = torch.rand(3, 3)\n",
    "tensor_on_gpu = torch.rand(3, 3, device=DEVICE)\n",
    "print(f\"Tensor\\n{tensor_on_cpu}\\n  is stored on {tensor_on_cpu.device}.\")\n",
    "print()\n",
    "print(f\"Tensor\\n{tensor_on_cpu}\\n  is stored on {tensor_on_gpu.device}.\")\n",
    "print()\n",
    "print(f\"We can also move between the devices with `.to(<device>)`tensor_on_cpu.to(DEVICE).device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ca1ea",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Learn PyTorch for Deep Learning: Zero to Mastery book, accessed online on 2023.03.25.\n",
    "\n",
    "[2] Papers with Code trends, accessed online on 2023.03.25."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
